This project presents a real-time American Sign Language (ASL) recognition and education system designed to bridge communication and learning gaps for hearing-impaired communities. The proposed method combines object detection (YOLOv8) and pose estimation (MediaPipe) to recognize isolated ASL gestures with 98.80% accuracy at 56.2 FPS. The system provides three levels of feedback: (1) sign classification, (2) biomechanical error diagnosis with finger-specific corrections, and (3) ASL grammar validation for sentence formation. Five architectures (YOLOv8, ResNet18, EfficientNet-B0, MobileNetV2, MediaPipe+ML) were systematically compared on 18 ASL signs (1,105 training images, 70 test images). Statistical analysis across 5 independent training runs confirmed YOLOv8's superiority with large effect sizes (vs MediaPipe: t(4)=9.22, p<0.001, d=4.61; vs ResNet18: t(4)=3.78, p<0.05, d=1.89), demonstrating both statistical significance and practical robustness (98.81% Â± 0.68% mean accuracy). The hybrid architecture operates on consumer hardware (NVIDIA RTX 3050 Ti), making ASL education more accessible. This work advances sign language recognition by combining real-time detection with pedagogically-grounded,
biomechanical feedback.
